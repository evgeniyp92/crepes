# How Node works

## Preamble 1: What actually happens when we access a webpage?

Our browser (client) sends a request to the server and the server responds with the webpage

This is client-server architecture

1. DNS Lookup
2. Establish TCP/IP socket connection
3. HTTP request - method (`GET`), target (`/resource`), version (`HTTP/1.1`)
4. HTTP response - version (`HTTP/1.1`), status code (`200`), status message (`OK`)

Once the index.html is received it is scanned for assets that are required
and additional requests made for any dependant files

Backends come in several varieties

- Static (Server serves files)
- Dynamic (Server talks with apps and serves files, as well as communicates with databases)
- API

Common choices for backend is node and mongodb  
Fullstack is simply the frontend and backend combined into one  
Node is versatile but in the context of this we will be using it for webdev

---

## Preamble 2: Static vs Dynamic websites

- Static

  - The developer uploads files onto the webserver and they are served as-is
  - Note that just because something has JS does not make it dynamic

- Dynamic

  - Database is fetched from the data and websites are built from the template and the db data
  - This is called server-side rendering (SSR)

- APIs

  - Because browsers are becoming more and more powerful we can use API's to fill data at the client
  - We get data from a database and send it to the front end as a JSON
  - The browser then receives(consumes) it and then does things with it on the front end
  - Typically you would use some kind of frontend framework (like React.js) to consume the data effectively

Node is great for building API's, but you could also build Dynamic websites if you so choose  
This course will cover both!  
A key benefit of the API is that it can be consumed by not just browsers but anything.  
This is a key advantage of using API's over SSR  
One api, one data source for browser, mobile, desktop, other servers etc etc etc

---

## Node, V8, Libuv and C++

Node has a couple of libraries it depends on to run properly

- V8 - The Javascript engine that powers node under the hood - Written in JS and C++
- Libuv - OS library with strong focus on async I/O - this is what makes node chooch outside the browser (file system access, networking, etc), it also implements the event loop and the thread pool - Written in C++
- http-parser
- c-ares
- openSSL
- zlib

---

## Processes, threads and the thread pool

When we use node we create a node process on the computer

This actually gives us access to a process variable

The Node process runs in a single thread, which makes it easy to block Node applications

### Startup sequence

- Program is initialized
- Top-level code is executed
- Required modules are brought in
- Event callbacks registered
- Event loop starts

Some tasks are too heavy to be executed in the event loop, which is where the thread pool comes in.  
This gives us 4 additional threads (up to 128), which can take heavy tasks away from the event loop.  
This process is opaque and we cannot control this. File operations, compression, dns lookups and cryptography are examples of things that are 'heavy'

---

## The Event Loop

The event loop contains all application code inside callback functions (non-top-level code)  
Node is built around callback functions with an event driven architecture

- Events are emitted
- Event loop picks them up
- Callbacks are called

Examples can be:

- New HTTP request
- Timer expired
- Finished file reading

The event loop does orchestration to offload expensive tasks to the thread pool

When we start the app the loop starts running right away  
Node has many callback queues and the loop has many phases

- Expired timer callbacks (2 queues)
- I/O polling and callbacks (4 queues)
- setImmediate callbacks (1 queue) - for when you want to execute something right after I/O polling
- Close callbacks (2 queues)

- Process.nextTick() queue
- Other microtasks like resolved promises

The last two execute at the next available opportunity in the event loop  
The loop continues until there are no more pending timers or I/O tasks  
The event loop makes async programming possible in node

Node is singlethreaded unlike php which spawns a new thread for every user

### Avoid blocking the main thread as much as possible

Don't use sync versions of functions in fs, crypto, zlib in callback  
Don't do complex math (loops inside loops)  
Be careful with JSON in large objects  
Don't use overly complex regex (i.e. nested quantifiers)

---

## Event loop in practice

```javascript
// this executes second
setTimeout(() => console.log('Timer 1 finished'), 0);

// this executes third
setImmediate(() => console.log('Immediate timer finished'));

// this executes fourth
fs.readFile('test-file.txt', () => {
  console.log('I/O finished');
});

// this executes first
console.log('Hello from top-level code');
```

### Setting the threadpool size

```javascript
process.env.UV_THREADPOOL_SIZE = desiredThreadCount;
```

---

## Events and event-driven architecture

Most of nodes core modules are built around an event-driven architecture  
We can use this same architecture to our own advantage in our code with event emitters and event listeners which have callbacks attached to them

This pattern is called the observer pattern in JS.  
Under this pattern, things are much more decoupled and modular

With the observer pattern we can set up many things to fire at once in response to a single event

```javascript
// setup
const EventEmitter = require('events');
const myEmitter = new EventEmitter();

// listeners
myEmitter.on('newSale', () => console.log('There was a new sale'));
myEmitter.on('newSale', () => console.log(`ðŸ’©`));

// emitter
myEmitter.emit('newSale');
```

Adding additional data with the emitter

```javascript
myEmitter.on('newSale', stock => {
  console.log(`There are now ${stock} items left`);
});

myEmitter.emit('newSale', 9);

// there are now 9 items left
```

Extending a class

```javascript
const EventEmitter = require('events');

class Sales extends EventEmitter {
  constructor() {
    super();
  }
}

const myEmitter = new Sales();
```

In a lot of the prepacked modules there's a lot of class extension going on

---

## Streams

Streams are used to process data piece by piece without completing the whole read/write, therefore keeping all the data in memory  
This makes them perfect for handling large volumes of data and makes the task of data processing more efficient, because we dont need to keep the entire thing in memory

In node there are 4 fundamental streams (streams are actually instances of EventEmitter)

- Readable
  - From which we can read data
  - http requests
  - fs read streams
  - important events: `data` and `end`
  - most useful functions are `pipe()` and `read()`
- Writable
  - To which we can write data
  - http responses
  - fs write streams
  - important events: `drain` and `finish`
  - `write()`, `end()`
- Duplex
  - Readable and writeable at the same time
  - Like web sockets (communication channel that works 2-way)
- Transform
  - Duplex streams that can modify/transform data as it is written/read
  - like zlib Gzip creation

We can also implement our own streams and consume them but that's a subject for a later point

```javascript
//  reading a large file with streams
const fs = require('fs');
const server = require('http').createServer();

server.on('request', (req, res) => {
  // most simple: reading a file, storing in a var and sending to client
  fs.readFile('test-file.txt', (err, data) => {
    if (err) {
      console.log(err);
    }
    res.end(data);
  });
});

server.listen(8000, '127.0.0.1', () => {
  console.log(`server listening on port 8000`);
});
```

```javascript
server.on('request', (req, res) => {
  // using streams
  const readable = fs.createReadStream('test-file.txt');
  readable.on('data', chunk => {
    res.write(chunk);
  });
  readable.on('end', () => {
    res.end();
  });
  readable.on('error', err => {
    console.log(err);
    // @ts-ignore
    res.statusCode(500);
    res.end('File not found');
  });
});
```

```javascript
server.on('request', (req, res) => {
  // using streams and avoiding back pressure from the readStream
  const readable = fs.createReadStream('test-file.txt');
  readable.pipe(res);
  // readableSource.pipe(writableDestination)
});
```

---

## How requiring modules actually works

Each js file is a separate module  
Node uses the CommonJS system - `require()`, `exports` or `module.exports`  
ES module system is used in browsers - `import/export`  
There have been attemps to bring ES modules to node (`.mjs`)

### Requiring a module

- `require()` function is called
- Path to module gets resolved
  - Starts with core modules
  - Try to load dev modules if path begins with `./` or `../`
  - Try to find folder with `index.js` in it
  - Go to `node_modules/` and try to find module there
- File is loaded
